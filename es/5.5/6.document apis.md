document api ：针对指定文档的api

# reading and writing documents

本节主要介绍es的复制模型，并讨论在写和读的各种交互的操作时，它有怎样的影响。

## basic write model

略

# index api

在指定的索引下增加或者更新json格式的文档。

```json
PUT twitter/tweet/1
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
```

```json
{
    "_shards" : {
        "total" : 2,
        "failed" : 0,
        "successful" : 1
    },
    "_index" : "twitter",
    "_type" : "tweet",
    "_id" : "1",
    "_version" : 1,
    "created" : true,
    "result" : created
}
```

total：录入该文档，涉及到的分片数量（包括主分片和副本分片）。

successful：成功录入的分片数量。

failed：失败的分片数量。

**注意：**如果只开启一个节点，且只有一个节点，并且索引配置如下：

```json
{
    "settings" : {
        "index" : {
            "number_of_shards" : 3, 
            "number_of_replicas" : 4
        }
    }
}
//录入一个文档后，返回如下：
{
    "_shards" : {
        "total" : 5,
        "failed" : 0,
        "successful" : 1
    }
}
```

已知只有一个节点，所以该节点都是主分片，所有的副本分片都没有启动。

total：是基于number_of_replicas，因此是4+1（4个副本+1个主分片）

failed：没有失败，主分片成功了，而没有副本分片，因此不算入失败。

successful：只有主分片并且成功。

## automatic index creation

如果之前没有创建索引，那么以上操作会自动创建索引。

如果之前没有创建映射，那么以上操作会自动创建映射，同时映射是非常灵活无结构的。会根据新的字段或者对象自动创建映射。

禁止自动创建：在所有的节点的配置文件中加上：

`action.auto_create_index`

`index.mapper.dynamic`

同时索引创建支持黑/白名单。

`action.auto_create_index`： `+aaa*,-bbb*,+ccc*,-*`

## versioning

每一个被index的文档都会给定一个版本。他会作为index api返回的结果字段返回。通过指定version参数来进行乐观并发控制。执行一个事务“读后写”的案例：先读取文档，并在更新时指定读取的文档返回的版本，如果修改成功。可以保证在修改期间文档没有发生改变。（当为了更新而读，最好是将`preference`设置为`_primary`）

```json
//更新指定版本
PUT twitter/tweet/1?version=2
{
    "message" : "elasticsearch now has versioning support, double cool!"
}
```

如果没有提供版本，那么执行操作时不会进行任何版本检查。

默认，内部版本从1开始，并且每次写操作（更新，删除）都会使version递增。同时version支持外部版本（数据库同步）。为了开启这个功能，`version_type`必须被设置为`external`。值必须为数值，并且大于等于0.并且小于`9.2e+18`。当使用外部版本，不会去检查当前请求指定的版本是否和存储的文档的版本一致，而只要求指定版本比存储的版本大即可。如果成功，那么指定版本的文档会被index，并且指定版本作为新的存储版本值。

警告：外部版本支持0作为有效值，因为外部版本系统允许从0开始（数据库）。但是版本为0的文档不能被`update by query api`更新，也不能被`delete by query api `删除。

## version types

- `internal`：只有指定的版本和存储的文档版本一致，才会index文档。
- `external` or `external_gt`：只有指定的版本高于存储文档版本或者文档不存在，指定的版本作为新版本值并index文档。
- `external_gte`：大于或等于。文档不存在同样可以。

`external_gte`必须被谨慎使用，它可能导致丢失数据。（不懂）

## operation type

`PUT twitter/tweet/1` 默认是覆盖。

可以通过参数`op_type`，只有在不存在时才创建，存在时失败。

```json
PUT twitter/tweet/1?op_type=create
PUT twitter/tweet/1/_create
```

## automatic id generation

如果没有指定id，id会自动生成。

```json
POST twitter/tweet/
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
```

## routing

```json
POST twitter/tweet?routing=kimchy
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
```

默认一个文档的录入，会根据id来计算hash值，然后再确定会被放入到指定的分片上。如果指定`routing`，那么会根据这个`routing`值来替换id值计算来存放。

routing也可以在映射中指定。

作用：如果一个查询

```json
GET /testindex/_doc/_search
{
    "query":{
        "match":{
            "content":"get a dog"
        }
    }
}
```

该查询会将请求广播到所有分片，然后各自查完再返回协调节点，整合后再返回给用户。

如果一开始在录入时就指定了这类文档的位置，

`POST /testindex/_doc?routing=xxxx`

那么在查询时这样指定：

`GET /testindex/_doc/_search?routing=xxxx`

那么就不需要广播到所有分片，而是直接在计算得到的指定分片下去查找。可以提高效率，减少性能开销。

缺点：文档分布不均匀。

## parents & children

```json
PUT blogs
{
  "mappings": {
    "tag_parent": {}, //父类型
    "blog_tag": {     // 子类型
      "_parent": {    //在子类型指定其父
        "type": "tag_parent"
      },
       "properties":{}
    }
  }
}

PUT blogs/blog_tag/1122?parent=1111
{
    "tag" : "something"
}
```

当录入子文档时，其routing值自动被设置为与其父一致。除非有明确指定了routing值。

## distributed

录入操作基于路由被直接对准主分片并且在拥有该主分片的节点上执行。等到主分片完成操作后，如果需要，他会更新到副本分片上。

## wait for active shards

为了系统写的弹性，可以配置一个参数。使得写操作只有指定数量的分片是活跃的，才会去执行。默认是主分片活跃即可。

在索引上配置：`index.write.wait_for_active_shards`。

在请求url上配：`wait_for_active_shards`

其值为all 或者 1<=x<number_of_replicas+1 。

会在执行写请求前，去判断当前索引的活跃分片是否达到数量了。如果没有，则会进行等待直到超时。

因为检查是在写操作之前。如果检查时活跃分片是符合，但是执行时有活跃分片有问题。那么也会导致写的分片数量与预期不符合。

## refresh

具体看refresh

## noop updates

当使用index api更新文档时（put 替换指定id的文档），即使文档没有改变内容，它的version值也会总是被更新。

如果是 update api，可以使用`detect_noop`值去设置。而index api不适用，因为index api不回去获取旧资源，因此也就不会比较文档是否变更了。



## timeout

当执行索引操作时，被分配执行索引操作的主分片可能不可用。例如：主分片正从网关恢复或者处理迁移状态。默认情况下，索引操作会等待主分片最多一分钟。这个值可以被设置：

```json
PUT twitter/tweet/1?timeout=5m
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
```

# get api

通过文档id获取指定的文档。

`GET twitter/tweet/0`

可以通过head请求来判断文档是否存在

`HEAD twitter/tweet/0`

## realtime

get api 会就地触发refresh，因此即使文档更新了，但是没有refresh。在get api操作时，更新的文档也是可见的。可以使用`realtime=false`去禁止此操作。

## optional type

可以使用`_type`去指定搜索的index下的type。`_type=_all`会在所有的type中获取第一个匹配到id的文档。

## source filtering

get api 默认会返回source字段，除非指定了stored_fields参数或者 _source被禁用。

`_source=false` 不返回source，如果不需要文档内容，使用该参数可以减少网络开销。

`_source_include`：包含

`_source_exclude`：不包含

`_soucre`：包含或者是否要返回_source字段

用逗号分隔开列表，可以使用通配符表达式

```json

GET twitter/tweet/0?_source_include=*.id&_source_exclude=entities

GET twitter/tweet/0?_source=*.id,retweeted
```

## stored fields

指定返回`stored_fields`，那么不会返回`_source`，同时被指定的字段在mapping中必须是`store=true`，否则返回时会被忽略。

```json
PUT twitter
{
   "mappings": {
      "tweet": {
         "properties": {
            "counter": {
               "type": "integer",
               "store": false
            },
            "tags": {
               "type": "keyword",
               "store": true
            }
         }
      }
   }
}

PUT twitter/tweet/1
{
    "counter" : 1,
    "tags" : "red"
}

GET twitter/tweet/1?stored_fields=tags,counter

{
   "_index": "twitter",
   "_type": "tweet",
   "_id": "1",
   "_version": 1,
   "found": true,
   "fields": {
      "tags": [
         "red"
      ]
   }
}
```

`stored_field`字段返回的都是数组的形式，同时由于counter在mapping并没有被指定为`store=true`，因此返回结果没有它。

Also only leaf fields can be returned via the `field` option. So object fields can’t be returned and such requests will fail

不懂：`leaf fields` 具体是什么字段

## getting the _source directly

```json
//只返回source字段
GET twitter/tweet/1/_source
//只返回source字段的同时，指定只返回source中的指定字段。
GET twitter/tweet/1/_source?_source_include=*.id&_source_exclude=entities'
//判断source是否存在。
HEAD twitter/tweet/1/_source
```

source不存在的情况只有：在mapping中如下配置

```json
PUT tweets
{
  "mappings": {
    "tweet": {
      "_source": {
        "enabled": false
      }
    }
  }
}
```

## routing

如果index 文档时，使用了routing。那么get 文档时，同样需要指定routing。如果routing不一致，那么会导致搜索不到该文档。

## preference

get api默认从副本分片中随机查。可以修改这个机制。

- preference=_primary ：只从主分片查。
- preference=_local：如果可以的话（当前协调节点就存有数据u送在的分片），直接从当前节点进行查。
- custom (string) value ： 不懂。A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with "jumping values" when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name

## refresh

在get之前，刷新相关分片。使得所有文档可见。

## distributed

get操作会先根据routing/id计算hash值得到分片的id，然后会重定向到分片上去获取并返回结果。这个分片是这个分片id组中的其中一个。因此如果分片有更多的副本，那么将会有更好的扩展性。

## versioning support

# delete api

删除指定id的文档。

`DELETE /twitter/tweet/1`

## version

当删除文档时，可以指定一个version。以确保在删除文档期间，该文档没有被其他操作所修改。因为对文档的任何写操作（包括删除），都会导致它的version递增。

## routing

`DELETE /twitter/tweet/1?routing=kimchy`

如果文档录入时，有指定routing，那么删除时，也必须指定routing。否则会导致删除文档失败。

同时，如果mapping指定了routing参数为需要，但是操作时没有指定routing，那么delete api会抛出`RoutingMissingException`并拒绝请求。

## parent

parent参数可以被设置，同时它基本上和routing参数是一致的。

删除父文档并不会自动删除子文档。删除所有子文档可以通过使用delete by query api 。。。

删除子文档时，必须指定父id。否则删除将会被拒绝并抛出错误`RoutingMissingException`

## automatic index creation

delete api在执行时，如果没有数据中没有index和type。那么会自动创建该index和type。

## distributed

删除操作将会先通过hash获取分片id，然后在该id组中找到对应的主分片进行删除后。如果需要会同步到其他分片。

## wait for active shards

删除请求，可以指定`wait_for_active_shards`参数，在删除前，等待当前活跃的分片数达到指定数量，才去执行删除操作。

## refresh

刷新

## timeout

超时

# update api

该api允许基于提供的脚本去更新文档。它将从索引获取文档，执行脚本，将结果索引回。它通过version去确保在get和reindex之间没有发生写操作。

该操作依旧是文档的重新索引。它仅仅减少一些网络回返，在get和index之间减少版本冲突。同时source必须为enable。否则无法使用脚本更新。如下：

```json
PUT test/type1/1
{
    "counter" : 1,
    "tags" : ["red"]
}
//counter+4
POST test/type1/1/_update
{
    "script" : {
        "inline": "ctx._source.counter += params.count",
        "lang": "painless",
        "params" : {
            "count" : 4
        }
    }
}
//tags增加blue元素
POST test/type1/1/_update
{
    "script" : {
        "inline": "ctx._source.tags.add(params.tag)",
        "lang": "painless",
        "params" : {
            "tag" : "blue"
        }
    }
}
```

以上案例，挂载在ctx上的对象还有： `_index`, `_type`, `_id`, `_version`, `_routing`, `_parent`, and `_now` (the current timestamp)

```json
//新增字段
POST test/type1/1/_update
{
    "script" : "ctx._source.new_field = 'value_of_new_field'"
}

//删除某个字段
POST test/type1/1/_update
{
    "script" : "ctx._source.remove('new_field')"
}

//表达式
POST test/type1/1/_update
{
    "script" : {
        "inline": "if (ctx._source.tags.contains(params.tag)) { ctx.op = 'delete' } else { ctx.op = 'none' }",
        "lang": "painless",
        "params" : {
            "tag" : "green"
        }
    }
}
```

以上的脚本操作，如果指定文档不存在将会失败。

## detecting noop updates

如果doc被指定与已存在的文档进行合并，默认下不改变任何的更新的操作将会被忽略并且返回 `result:noop`

```json
//原始数据
PUT test/type1/1
{
    "name" : "new_name"
}
//无效更新
POST test/type1/1/_update
{
    "doc" : {
        "name" : "new_name"
    }
}
//结果
{
   "_shards": {
        "total": 0,
        "successful": 0,
        "failed": 0
   },
   "_index": "test",
   "_type": "type1",
   "_id": "1",
   "_version": 6,
   "result": noop
}
```

可以关闭掉此检测。

```json
POST test/type1/1/_update
{
    "doc" : {
        "name" : "new_name"
    },
    "detect_noop": false
}
```

## updates with  a partial document

update api支持传递部分文档，它将会与存在的文档进行合并。（简单递归合并，对象内部合并，直接替换 key/valuse 和数组）

```json
POST test/type1/1/_update
{
    "doc" : {
        "name" : "new_name"
    }
}
```

如果文档1不存在，那么报错。如果doc和script同时指定，那么doc会被忽略。

## doc_as_upsert

使用doc元素，如果文档不存在会报错。那么加上`doc_as_upsert`可以使得文档不存在时进行新增。

```json
POST test/type1/1/_update
{
    "doc" : {
        "name" : "new_name"
    },
    "doc_as_upsert" : true
}
```

## upserts

```json
POST test/type1/1/_update
{
    "script" : {
        "inline": "ctx._source.counter += params.count",
        "lang": "painless",
        "params" : {
            "count" : 4
        }
    },
    "upsert" : {
        "counter" : 1
    }
}
```

如果文档存在，那么脚本被执行。如果文档不存在，那么upsert作为文档被插入。

## scripted_upsert

没懂，If you would like your script to run regardless of whether the document exists or not — i.e. the script handles initializing the document instead of the `upsert` element — then set `scripted_upsert` to `true`:

```json
POST sessions/session/dh3sgudg8gsrgl/_update
{
    "scripted_upsert":true,
    "script" : {
        "id": "my_web_session_summariser",
        "params" : {
            "pageViewEvent" : {
                "url":"foo.com/bar",
                "response":404,
                "time":"2014-01-01 12:32"
            }
        }
    },
    "upsert" : {}
}
```

## Parameters

update操作还支持以下参数

https://www.elastic.co/guide/en/elasticsearch/reference/5.5/docs-update.html#_parameters_2

# bulk api

bulk api使得单个api被调用时可以执行更多的操作。这大大增加了索引的速度。

它以`_bulk`为终止点，并期望以下的json结构。

```json
action_and_meta_data\n
optional_source\n
action_and_meta_data\n
optional_source\n
....
action_and_meta_data\n
optional_source\n
```

文本内容，其最后一行必须是换行符结束，每一个换行符前面可以有一个回车符。当发送请求给该终止点时，`Content-Type`必须设置为`application/x-ndjson`

可能的操作有`index,create,delete,update`

- create和index api中`op_type=create`的语义一致。只创建。如果index下的type已经存在该文档，那么会失败。
- index 和 index api中`op_type=index`的语义一致。存在则覆盖，不会导致失败。
- delete 语义和delete api一致
- update 可以在第二行指定 doc、upsert和脚本等，和update api的语义一致。

如果你提供一个文本文件去curl，你必须指定 `--data-binary` 而不是`-d` 。后者不会保留换行符。

```json
$ cat requests
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value1" }
$ curl -s -H "Content-Type: application/x-ndjson" -XPOST localhost:9200/_bulk --data-binary "@requests"; echo
{"took":7, "errors": false, "items":[{"index":{"_index":"test","_type":"type1","_id":"1","_version":1,"result":"created","forced_refresh":false}}]}
```

以下是一个正确顺序的批量命令的案例

```json
POST _bulk
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "2" } }
{ "create" : { "_index" : "test", "_type" : "type1", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_type" : "type1", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }
```

终止点可以为：`/{index}/_bulk`, and `{index}/{type}/_bulk`，当index和type被提供时，它们会作为默认值供给那些没有指定index和type的操作。

在这里的想法时使得处理尽可能地快，因此除了`action_meta_data`被接受节点所解析，一些操作会被交给其他节点的分片去处理。

批量操作的结果是一个包含各个操作的大json结构的结果。单个操作的失败不会影响其他剩余的操作。

这里没有一个确切的指标：一个批量操作所执行的操作数量。你应该实验不同的配置去寻找一个适合的负载点。

如果使用http api。确保客户端不会发送http chunks，它会使得速度变慢。

## versioning

不懂，没具体案例

每一个批量项目都可以使用`_version/version`包含一个版本值，它基于`_version`映射，自动按照 index/delete 的操作。同样支持`version_type/_verison_type`

## routing

同上，支持`_routing`/`routing`

## parent

同上，支持`_parent`/`parent`

## wait for active shards

支持`wait_for_active_shards`

## refresh

支持`refresh`

## update

bulk中的update支持以下选项：`doc (partial document), upsert, doc_as_upsert, script and _source`

```json
POST _bulk
{ "update" : {"_id" : "1", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "doc" : {"field" : "value"} }
{ "update" : { "_id" : "0", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "script" : { "inline": "ctx._source.counter += params.param1", "lang" : "painless", "params" : {"param1" : 1}}, "upsert" : {"counter" : 1}}
{ "update" : {"_id" : "2", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "doc" : {"field" : "value"}, "doc_as_upsert" : true }
{ "update" : {"_id" : "3", "_type" : "type1", "_index" : "index1", "_source" : true} }
{ "doc" : {"field" : "value"} }
{ "update" : {"_id" : "4", "_type" : "type1", "_index" : "index1"} }
{ "doc" : {"field" : "value"}, "_source": true}
```



# refresh

index、update、delete和bulk api 支持使用refresh去控制什么时候可见到该请求所作出的修改。

- refresh/ refresh=true：操作发生后，立即刷新相关的分片（主和副本）。因此更新后的文档可以立即在查询结果中。使用该值必须慎重考虑是否会导致低性能。
- refresh=wait_for：会等待该请求做出的修改 因 刷新变得可见 才会响应该请求。它并不会立即刷新，而是等待刷新发生。es自动刷新分片`index.refresh_interval`默认是1秒。或者手动调用[refresh](https://www.elastic.co/guide/en/elasticsearch/reference/5.5/indices-refresh.html)，或者在index、update、delete和bulk api使用refresh参数。都会导致刷新。进而导致refresh=wait_for的请求被响应。
- refresh=false/不使用该参数：不刷新。

## example

```json
//刷新
PUT /test/test/1?refresh
{"test": "test"}
PUT /test/test/2?refresh=true
{"test": "test"}

//不刷新
PUT /test/test/3
{"test": "test"}
PUT /test/test/4?refresh=false
{"test": "test"}

//等待刷新后再响应
PUT /test/test/4?refresh=wait_for
{"test": "test"}
```

## refresh=wait_for can force a refresh

如果当前分片上已经有`index.max_refresh_listeners`（默认1000）个请求在等待，那么再有一个refresh=wait_for请求进来，那么这个请求会被设置为refresh=true 进而触发刷新。同时这个请求的响应会包含`"forced_refresh": true`

每个bulk 请求只会对每个分片都只占用一个槽，不管他们对分片修改多少次。

## choosing which setting to use

- 如果对一个索引有很多改变，则使用wait_for比true更节省工作。
- true会创建低效的索引结构，后续会被合并到高效的索引结构。意味着true的开销在：创建低效索引结构的开销，在这个低效的索引结构查询的开销，合并到高效索引结构的开销。
- 不连续地使用refresh=wait_for的请求。而是将他们放入到balk请求中（使用refresh=wait_for）。es将会并行处理他们，并在他们全部完成后返回。
- 如果将`index.refresh_interval`设置为-1，则不会自动刷新，那么wait_for的请求只能等待手动调用[refresh](https://www.elastic.co/guide/en/elasticsearch/reference/5.5/indices-refresh.html)，或者在index、update、delete和bulk api使用refresh参数。刷新后才会响应。如果将`index.refresh_interval`设置过短，那么wait_for请求会很快响应。
- refresh=wait_for 只会影响它自己的请求，而true会影响其他正在进行的请求。因此如果你不希望打扰到系统上的其他请求，那么选择wait_for会较小的修改。

# delete by query api

```json
POST twitter/_delete_by_query
{
  "query": { 
    "match": {
      "message": "some message"
    }
  }
}

{
  "took" : 147,
  "timed_out": false,
  "deleted": 119,   //删除数量
  "batches": 1,     
  "version_conflicts": 0, //版本冲突
  "noops": 0,  //未更新
  "retries": {
    "bulk": 0,   //批量删除重试次数
    "search": 0  //查询重试次数
  },
  "throttled_millis": 0,
  "requests_per_second": -1.0,
  "throttled_until_millis": 0,
  "total": 119,
  "failures" : [ ] //所有的失败
}
```

`_delete_by_query`当开始时会生成索引的快照，并使用内部版本号去删除。如果文档在快照生成后，删除操作执行前被修改过，将会引起版本冲突。版本匹配的文档会被删除。

注意：因为内部版本不支持0的版本值，因此文档的版本号为0不会被`_delete_by_query`删除，同时请求会报错。

`_delete_by_query`被执行时，多个查询请求顺序执行。每批文档被查询到就会执行删除。一旦查询或删除被拒绝，`_delete_by_query`根据默认策略去重试被拒绝的请求（最高10次），达到临界值后将会终止。并且将失败返回在响应中的`failures`。已执行的删除操作依旧有效。换而言之，处理进程只会终止，不会回滚。当出现失败导致终止，之前所有的失败都会被放入到`failures`中。因此它可能会有很多失败的实体。



如果想计算版本冲突数量，而不是造成终止。可以设置：在url上：`conflicts=proceed`  或者 请求体上： `"conflicts": "proceed"`

`POST twitter/tweet/_delete_by_query?conflicts=proceed`



指定多个索引，多个类型：

```json
POST twitter,blog/tweet,post/_delete_by_query
```

 待补充，其他不懂。

# update by query api

好像不是更新文档数据，而是更新mapping。

类似`delete_by_query`，看不懂。

# reindex api



# multi get api

没有search api好用。不看。