一个数据库的pod，运行多个，每一个都需要单独的持久卷。这种场景是之前的资源对象都无法做到的。

解决：

1. 直接创建pod，而不是用ReplicaSet管理它。
2. 创建多个ReplicaSet，每个ReplicaSet管理一个pod。

集群应用要求每一个实例在集群生命周期内唯一标识。但是pod被新的pod删除，它是全新的主机名和ip。新启动的实例有新的网络标识，同时还用旧实例数据会引起问题。这个需求是：每个集群成员的配置文件种都列出其他集群成员和他们的ip。但是新pod，是会变化的。

解决：每一个pod都对应一个service管理。

综上：

![image-20201020223756852](E:\0git_note\docker\img\image-20201020223756852.png)

# Statefulset

它和ReplicaSet和ReplicationController类似，但又有不一样。

它保证了pod在重新调度后保留他们的标识和状态。同时每个pod可以拥有一组独立的持久卷。pod的命名时有规律固定的。

## 稳定的网络标识

![image-20201020231359803](E:\0git_note\docker\img\image-20201020231359803.png)

1.扩缩容时，扩容在之前的索引号+1，缩容，先缩索引值高的。

2.缩容只会操作一个pod实例，缩容不会很迅速，在有实例不健康的情况下，不允许缩容。

![image-20201020232515518](E:\0git_note\docker\img\image-20201020232515518.png)

![image-20201020232544339](E:\0git_note\docker\img\image-20201020232544339.png)

## 专属存储

持久卷声明和持久卷时一对一的。

持久卷声明被删除，那么持久卷也会被删除。因此statefulset缩容不会去删除持久卷声明。

statefulset每个pod都关联到不同的持久卷声明。

扩容时：创建两个或更多的api对象（pod和持久卷声明）

缩容：只会删除pod

![image-20201020232839801](E:\0git_note\docker\img\image-20201020232839801.png)

## 保障

![image-20201020232941788](E:\0git_note\docker\img\image-20201020232941788.png)

## 创建

创建三个持久磁盘和持久化卷。

```yaml
kind: List
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistenVolum
  metadata:
    name: pv-a
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    PersistenVolumeReclaimPolicy: Recycle
    gcePersistentDisk:
      pdName: pv-a
      fsType: nfs4
- apiVersion:
。。。
```

这里是通过kin=list来在一个yaml文件上创建多个资源，也可以通过---来区分资源。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  clusterIP: None //标识这个service是headless模式
  selector:
    app: kubia
  ports:
  - name: http
    port: 80
    
```

```yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data
          mountPath: /var/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      resources:
        requests:
          storage: 1Mi
        accessModes:
        - ReadWriteOnce
          
```

这里是创建持久化卷声明的模板，会根据每个pod创建一个持久化卷声明。同时不需要再pod上包含声明，因为StatefulSet会自动将声明卷添加到pod详述里。

**注意：**StatefulSet创建pod、扩容和缩容，都是一个一个地来，因为集群中同时启动会引起竞态条件。单独启动更加安全。

## 访问

通过headless无法通过service访问。这时候必须进入一个pod，在pod内去访问。

这里提供另一种方法。通过API服务与pod通讯。

```shell
<apiServerHost>:<port>/api/v1/namespces/default/pods/<podName>/proxy/<path>
```

因为API服务有安全保证，所以可以使用kubectl proxy来与API服务通信。

```shell
kubectl proxy
```

将使用返回地信息，代替实际地API服务地址和端口。

```shell
curl <host>:<port>/api/v1/namespces/default/pods/<podName>/proxy
```

![image-20201021231258678](E:\0git_note\docker\img\image-20201021231258678.png)

通过API服务访问服务service。

```shell
/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>
```



## 发现伙伴节点

通过dns的drv记录

## 更新

StatefulSet 更新是ReplicationController和ReplicaSet，所以更改镜像是不会改变原有的pod。（1.7之后好像可以）

## StatefulSet处理节点失效

因为要保证不会有两个拥有相同标记和存储地pod同时运行，当一个节点似乎失效时，Statefulset在明确知道一个pod不再允许之前，它不能或者不应该创建一个替换pod。

只有集群地管理者告诉它这些信息的时候，它才能明确知道。

### 模拟一个节点断开

```shell
sudo ifconfig eth0 down
```

当这个而节点的网络节点关闭后，其上的kubelet服务无法与k8s api服务通信，无法汇报本节点和上面的pod正常允许。

因此该节点被标记为NotReady。

该节点上的pod被标记为Unknown

```shell
kubectl describe po <podName>
```

可以看到这个pod是Termination，原因是NodeLost。

这里展示的是控制组件看到的信息，实际上这个pod对应的容器并没有被终止，还在正常运行。

### 手动删除pod

明确节点不会回来，同时想把这个节点上的statefulSet创建的pod调度到另一个健康节点上，就应该手动删除。

```shell
kubectl delete pod <podName>
```

但是这个pod并没有被删除。

```shell
kubectl get pod
```

从age就可以看出，如果是新的pod，age应该是几秒。

这个pod之前已经被标记为删除，因为通过describe中的status就知道。但是得它所在的节点上的kubelet通知api服务器说这个pod的容器已经终止，那么它才会被清除掉。但是这个节点是网络断开，所以无法通知。也就不会被清除。

### 强制删除

告诉api服务器不用等待kubelet确认，而是直接删除。

```shell
kubectl delete po <podName> --force --grace-period 0
```

