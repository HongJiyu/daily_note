# service

场景：pod基本是由controller托管的，尽管每个pod都有自己的ip，但是pod是变动的：扩缩容、节点宕机等情况，会导致pod变化，进而ip变化，因此pod的ip是无法作为固定提供给外部服务或内部pod访问访问的。先不考虑外部能否通过pod的ip访问到pod。

kubernetes服务（service）：为一组功能相同的pod提供单一不变的接入点：提供一个固定的ip。

![image-20201015232539756](img\image-20201015232539756.png)

前端服务：给集群外部提供统一的访问地点。

后端服务：防止pod变化而ip变化，给前端服务提供固定访问点。

服务的连接对所有的pod是负载均衡的。

## 创建服务

1.expose ，学习pod那章就用到了

```shell
kubectl expose <rc/rs/ds> kubia --type=LoadBalancer --name kubia-http
```

2. yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-service
spec: 
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

创建kubia-service服务，这个服务暴露自己的80端口，关联到含有app:kubia标签的所有pod的8080端口。

## 查看服务

```shell
kubectl get svc/service
```

**服务的主要目标是：提供唯一的ip，使得集群内部的其他pod访问这组pod，而非对外暴露服务。**

## 验证

```shell
kubectl exec <podName> -- curl -s http://ip
```

为一组pod创建服务后，通过kubectl get svc查看服务，获取到服务的ip后，进入到其他pod，并访问这个ip和端口。看看能否通过其他pod访问服务来访问这组pod。

**注意：** -- 代表kubectl命令项的结束。如果需要执行的命令没有以横杠（-）开始的参数，双横杠也就不是必须的。

## 服务会话亲和性

服务默认是随机将每个连接指向后端pod中的其中一个。

目前有两种选择：None、ClientIp，默认是None。

```yaml
apiVersion: v1
kind: Service
spec:
  sessionAffinity: ClientIP
```

ClientIP指 将同一个client IP的所有请求都转发到同一个pod

## 同一个服务暴露多个端口

服务：

```yaml
apiVersion: v1
kind: Service
metadata: 
  name: kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
```

这个服务同时监听这组pod的8080和8443端口，然后暴露自己的80和443端口，同时为自己暴露的端口起了http和https的名字。

pod：

```yaml
kind: Pod
spec: 
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
    - name: https
      containerPort: 8443
```

pod 为端口起了两个名字，这样在service的yaml中，可以直接指定名字，而不用指定端口。如果pod端口出现变动，就只需要重新建立pod，而不用更改service。

## 服务发现

客户端如何去发现服务。难道需要先启动服务，然后在客户端的配置中去配置服务的ip。这种方法就很耦合，k8s提供了发现服务的ip和端口的方式。

### 环境变量

任何一个pod，不管是否被托管，是否在某个服务下，只要pod开始运行，k8s会初始化一系列环境变量指向现在存在的服务。

即：pod创建了，k8s将现有的服务service的ip和端口，初始化到pod内容器的环境变量中。即env。

![image-20201016162504303](img\image-20201016162504303.png)

因此可以通过pod中的环境变量来获取服务的ip，前提是服务比pod先启动，否则就得删除pod，先启动服务，再启动pod，pod中才有对应的环境变量。

### DNS

好像在每个命名空间下都会有一个pod，称为xx-dns，这个pod用来运行dns服务，同时集群中的其他pod都被配置成使用k8s自身的dns服务器响应（/etc/resolv.conf）

**注意：**pod是否使用内部的dns服务是根据pod中的spec的dnsPlicy属性决定的。

因此集群内的pod在知道服务名称的情况下，可以通过**全限定域名**来访问，而不是通过环境变量。

总结：使用dns，就不用去考虑pod启动先后问题（得先启动service，获取到ip，再填入业务代码，再启动pod），而是直接指定全限定名即可。

#### 全限定名

`backend-database.default.svc.cluster.local`

backend-database：对应服务名（service名）

default：服务所在的命名空间

svc.cluster.local：是再所有集群本地服务名称中使用的可配置集群域后缀（不懂，但好像是固定的）

**注意：**客户端仍然需要知道服务的端口号，除非是标准端口：http的80等。

同时，如果同属于一个命名空间，那么命名空间和后缀都可以省略。具体看容器的/etc/resolv.conf。

尝试一下，在一个存在的pod的容器中，curl < serivceName:port>。

#### 无法ping通

通过curl < serviceName:port>，可以访问到服务

但是无法ping < serviceName> 这是因为服务的集群ip是一个虚拟ip，需要与服务端口结合才有意义。

## 连接集群外部服务

到目前，service提供了一个固定的ip，这个ip只能够在k8s集群内部使用。此时，k8s集群内和外部服务是还未互通的。

如何让k8s集群内部连接到外部服务：

### endpoint

首先 service和pod并不是有直接关系的，而是通过标签选择器匹配的，其实他们中间还有一个资源：endpoint。

`kubectl describe svc <serviceName>`

![image-20201016222001227](img\image-20201016222001227.png)

service 定义了标签选择器，然后选择到对应的pod，然后将这个service的ip 和 pod的ip关联在一起的资源就是endpoint。

endpoint在service定义了标签选择器，并且启动时，会自动创建endpoint资源。

**原理：如果service没有定义标签选择器，则启动时不会创建endpoint。这时候我们就可以为这个服务创建一个endpoint，来指定访问service后跳转到哪些ip上，这样，ip配置为外部服务的ip，那么就可以实现k8s集群内部访问外部资源。**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  ports:
  - port:80
```

这个service没有定义标签选择器，因此启动不会创建endpoint，这时候就可以手动创建endpoint

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 12.12.12.12
    ports:
    - port:80
```

配置中，endpoints的name必须要service的name一致。

这时候访问service的80端口，就会被重定向到endpoints指定的ip+port 上。

![image-20201016223316960](img\image-20201016223316960.png)

### ExternalName

除了使用endpoint来通过服务去访问外部方法，还可以通过完全限定域名访问呢外部服务。

设置service的type=ExternalName

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: someapi.somecompany.com
  ports:
  - port: 80
```

连接到服务的客户端将直接连接到外部服务，完全绕过服务代理。出于这个原因，这些类型的服务甚至不会获得集群ip。

## 将服务暴露给外部客户端

service的type有好几种类型：ClusterIP、NodePort、LoadBalance、ExternalName。

默认是ClusterIP：集群ip，集群内部使用，无法被外部访问。



![image-20201016223857315](img\image-20201016223857315.png)



### NodePort

常规的服务：type: ClusterIP ，这种服务只能够给集群内部访问。

而NodePort，不仅可以通过服务内部集群ip访问NodePort服务，还可以通过任何节点的IP和预留节点端口访问NodePort服务。

```yaml
apiVersion: v1
kind: Service
metadata: 
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort:8080
    nodePort: 30123
  selector:
    app: kubia
```

nodePort：节点暴露的端口

targetPort：pod暴露的接口

port：服务暴露的接口

nodePort不需要指定也可以，kubenetes将选一个随机端口，而且是每个节点都暴露这个端口。

以上端口的作用：

port： 通过服务ip+port 可以访问到pod （集群内部ip）

nodePort：访问节点ip + nodePort  可以访问到pod  

![image-20201016225547153](img\image-20201016225547153.png)

 通过节点端口访问之前，还需要配置防火墙，具体看： page139

minikube：

```shell
minikube service <serviceName> [-n <namespace>] #即可
```



### Load Balancer

它是通过创建一个外部负载均衡器。拥有自己独一无二的可公开的ip地址。它是nodeport的扩展，如果集群不支持Load Balancer，则不会调配负载平衡器，但他仍表现为nodeport服务一样。好像minikube就不支持。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

同样，没有指定节点port，那么k8s会选择一个端口。

`kubectl get svc kubia-loadbalancer`

可以看到他会有一个外部ip。即通过这个外部ip和80端口即可访问到pod。

![image-20201016230816731](img\image-20201016230816731.png)

### 缺陷

1.外部服务访问节点，节点再将请求转发给pod。请求到节点时，并不一定会将请求转发给这个节点的pod。如果转发给其他节点的pod，那会多一层的网络跳转。

解决：

```yaml
spec:
  externalTrafficPolicy: Local
```

但是这种解决也有缺点：如果当前节点无pod，那么请求会被挂起，而不会转发到有pod的节点。

同时会导致pod负载不均衡，如果：

![image-20201016231528460](img\image-20201016231528460.png)

2.集群内部访问服务，服务的pod可以获取到客户端的ip，但是集群外部通过节点端口请求，由于对数据包执行了源网络地址转换（nat），源ip发生改变。

## 总结

service的type有四种：

ClusterIP：创建集群内部访问的服务。

ExternalName：将服务映射到dns，访问外部服务。

NodePort：为集群外部提供服务

LoadBalancer：nodeport的扩展，有一个负载均衡器提供外部ip供外部服务访问。

# Ingress

同样可以将服务暴露给外部，但他不属于service。

ingress有点像nginx的代理。

![image-20201016232543443](img\image-20201016232543443.png)

ingress和loadbalancer一样，有限制，它必须有ingress控制器在集群中运行，ingress资源才能正常工作。

minikube包含一个可以启用的附加组件，可以使用ingress功能。

```shell
minikube addons list #查看附加组件启动情况
minikube addons enable ingress #启动ingress
minikube get pod --all-namespaces #启动后的ingress，好像会在每个命名空间下创建一个pod，提供ingress功能，和kube-dns一样。 
```

```yaml
apiVersion: extendsions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
```

在节点（主机）通过访问kubia.example.com，会被转化到 kubia-nodeport服务上。

但是，要确保主机得域名解析为ingress控制器的ip。

`kubectl get ingresses`

查找到ip后，在所有节点上配置host，即可在所有节点上访问到service服务。

## ingress暴露多个服务

```yaml
apiVersion: extendsions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
      - path: /port1
        backend:
          serviceName: kubia-nodeport1
          servicePort: 80
  - host: foo.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: foo-nodeport
          servicePort: 80
```

同样的，需要在节点上配置host文件。将

kubia.example.com 和 foo.example.com 与ingress的ip对应起来。

| 访问                    | 实际                |
| ----------------------- | ------------------- |
| kubia.example.com       | kubia-nodeport服务  |
| kubia.example.com/port1 | kubia-nodeport1服务 |
| foo.example.com         | foo-nodeport服务    |

因为rules和paths是复数，因此可以配置多个host，同个host也可以配置多个path。

## 配置Ingress处理TLS传输

![image-20201017110317803](F:\note\docker\img\image-20201017110317803.png)

简单说，控制器和后端pod不需要支持tls。只需要让客户端和控制器建立tls，让控制器处理和tls相关内容即可。让控制器做到这点，需要将证书和密钥附加到ingress中。

创建私钥和证书

```shell
openssl genrsa -out tls.key 2048
openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com
kubectl create secret tls tls-secret --vert=tls.cert --key=tls.key
```

```yaml
apiVersion: extendsions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  tls:
  - hosts:
    - kubia.example.com
    secretName: tls-secret
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
```

在spec增加了tls属性，即收到来自kubia.example.com主机的tls连接，从tls-secret中获取之前建立的私钥和证书。

调用`kubectl apply -f xx.yaml`更新ingress资源，而不是删除并从新文件重新创建的方式。

# pod就绪后发出的信号

pod在被创建后，如果还未就绪，那么就不会有请求转发给它。如果我们希望应用程序做一些提前的工作（缓存预热等），使得这些提供工作做完，然后pod再处于就绪状态，而不是让pod的启动，能够运行，就算是就绪。

因此需要就绪探针的存在。学习pod时有存活探针，现在是就绪探针。

就绪探针确保只有准备好处理请求的pod才可以接受请求。

![image-20201017114538831](F:\note\docker\img\image-20201017114538831.png)

## 添加就绪探针

`kubectl edit rc kubia`

```yaml
...
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        readinessProbe:
          exec:
            command:
            - ls
            - /var/ready
```

这个就绪探针用于判断容器内部是否存在 /var/ready文件。如果不存在，那么这个pod就是running，但一直处于ready。

`kebectl  exec <podName> --toch /var/ready`

到pod容器上创建这个文件，那么重新查看pod状态，就会变为ready。

就绪探针的执行周期等信息：`kubectl describe` 查看

![image-20201017115216472](F:\note\docker\img\image-20201017115216472.png)

## 就绪探针的作用

pod中的应用依赖于连接MySQL服务和redis服务，那么可以给pod添加就绪探针，尝试访问到mysql和redis服务都启动了，那么服务才就绪。

# headless服务

感觉没啥用处，具体看：page155