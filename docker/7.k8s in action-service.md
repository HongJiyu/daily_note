# service

场景：pod基本是由controller托管的，尽管每个pod都有自己的ip，但是pod是变动的：扩缩容、节点宕机等情况，会导致pod变化，进而ip变化，因此pod的ip是无法作为固定提供给外部服务或内部pod访问访问的。先不考虑外部能否通过pod的ip访问到pod。

kubernetes服务（service）：为一组功能相同的pod提供单一不变的接入点：提供一个固定的ip。

![image-20201015232539756](img\image-20201015232539756.png)

前端服务：给集群外部提供统一的访问地点。

后端服务：防止pod变化而ip变化，给前端服务提供固定访问点。

服务的连接对所有的pod是负载均衡的。

## 创建服务

1.expose ，学习pod那章就用到了

```shell
kubectl expose <rc/rs/ds> kubia --type=LoadBalancer --name kubia-http
```

2. yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-service
spec: 
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

创建kubia-service服务，这个服务暴露自己的80端口，关联到含有app:kubia标签的所有pod的8080端口。

## 查看服务

```shell
kubectl get svc/service
```

**服务的主要目标是：提供唯一的ip，使得集群内部的其他pod访问这组pod，而非对外暴露服务。**

## 验证

```shell
kubectl exec <podName> -- curl -s http://ip
```

为一组pod创建服务后，通过kubectl get svc查看服务，获取到服务的ip后，进入到其他pod，并访问这个ip和端口。看看能否通过其他pod访问服务来访问这组pod。

**注意：** -- 代表kubectl命令项的结束。如果需要执行的命令没有以横杠（-）开始的参数，双横杠也就不是必须的。

## 服务会话亲和性

服务默认是随机将每个连接指向后端pod中的其中一个。

目前有两种选择：None、ClientIp，默认是None。

```yaml
apiVersion: v1
kind: Service
spec:
  sessionAffinity: ClientIP
```

ClientIP指 将同一个client IP的所有请求都转发到同一个pod

## 同一个服务暴露多个端口

服务：

```yaml
apiVersion: v1
kind: Service
metadata: 
  name: kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
```

这个服务同时监听这组pod的8080和8443端口，然后暴露自己的80和443端口，同时为自己暴露的端口起了http和https的名字。

pod：

```yaml
kind: Pod
spec: 
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
    - name: https
      containerPort: 8443
```

pod 为端口起了两个名字，这样在service的yaml中，可以直接指定名字，而不用指定端口。如果pod端口出现变动，就只需要重新建立pod，而不用更改service。

## 服务发现

客户端如何去发现服务。难道需要先启动服务，然后在客户端的配置中去配置服务的ip。这种方法就很耦合，k8s提供了发现服务的ip和端口的方式。

### 环境变量

任何一个pod，不管是否被托管，是否在某个服务下，只要pod开始运行，k8s会初始化一系列环境变量指向现在存在的服务。

即：pod创建了，k8s将现有的服务service的ip和端口，初始化到pod内容器的环境变量中。即env。

![image-20201016162504303](F:\note\docker\img\image-20201016162504303.png)

因此可以通过pod中的环境变量来获取服务的ip，前提是服务比pod先启动，否则就得删除pod，先启动服务，再启动pod，pod中才有对应的环境变量。

### DNS

好像在每个命名空间下都会有一个pod，称为xx-dns，这个pod用来运行dns服务，同时集群中的其他pod都被配置成使用k8s自身的dns服务器响应（/etc/resolv.conf）

**注意：**pod是否使用内部的dns服务是根据pod中的spec的dnsPlicy属性决定的。

因此集群内的pod在知道服务名称的情况下，可以通过**全限定域名**来访问，而不是通过环境变量。

总结：使用dns，就不用去考虑pod启动先后问题，而是直接指定全限定名即可。

#### 全限定名

`backend-database.default.svc.cluster.local`

backend-database：对应服务名（service名）

default：服务所在的命名空间

svc.cluster.local：是再所有集群本地服务名称中使用的可配置集群域后缀（不懂，但好像是固定的）

**注意：**客户端仍然需要知道服务的端口号，除非是标准端口：http的80等。

同时，如果同属于一个命名空间，那么命名空间和后缀都可以省略。具体看容器的/etc/resolv.conf。

尝试一下，在一个存在的pod的容器中，curl < serivceName:port>。

#### 无法ping通

通过curl < serviceName:port>，可以访问到服务

但是无法ping < serviceName> 这是因为服务的集群ip是一个虚拟ip，需要与服务端口结合才有意义。